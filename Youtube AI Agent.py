# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1acBQFmHvI9yjHzP4vNopTG1y5QaPO-RM
"""

!pip install -q streamlit langchain-community langchain-core langchain-huggingface faiss-cpu sentence-transformers youtube-transcript-api

!npm install -g localtunnel

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py

import streamlit as st
import os
from langchain_community.document_loaders import YoutubeLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEndpoint

if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None
if 'llm' not in st.session_state:
    st.session_state.llm = None

with st.sidebar:
    st.header("Setup")
    hf_token = st.text_input("HuggingFace Token", type="password")
    url = st.text_input("Enter URL")
    process = st.button("Process")

if process and hf_token and url:
    os.environ["HUGGINGFACEHUB_API_TOKEN"] = hf_token

    with st.spinner("Loading..."):

        docs = YoutubeLoader.from_youtube_url(
            url,
            add_video_info=True
        ).load()

        chunks = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        ).split_documents(docs)

        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        st.session_state.vectorstore = FAISS.from_documents(chunks, embeddings)

        st.session_state.llm = HuggingFaceEndpoint(
                repo_id="mistralai/Mistral-7B-Instruct-v0.2",
                temperature=0.3,
                max_new_tokens=512
            )
        st.success("Processing complete!")

if st.session_state.vectorstore is not None:
    st.header("Ask a Question About the Video")
    question = st.text_input("Enter your question:")

    if st.button("Get Answer") and question:
        with st.spinner("Thinking..."):

            retriever = st.session_state.vectorstore.as_retriever(search_kwargs={"k": 3})
            docs = retriever.invoke(question)
            context = "\n\n".join([d.page_content for d in docs])

            prompt = f"""Answer based only on the context below. If unsure, say "I don't know".

            Context: {context}

            Question: {question}

            Answer:"""
            answer = st.session_state.llm.invoke(prompt)
            st.write("**Answer:**", answer)
else:
    st.info("Process a URL first")

!streamlit run app.py &

!npx localtunnel --port 8501

